{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13310058,"sourceType":"datasetVersion","datasetId":8437138},{"sourceId":13541726,"sourceType":"datasetVersion","datasetId":8599666}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adamdandi/2023-2025-training-performance-analysis?scriptVersionId=273647289\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **2023–2025 Training Performance Analysis: Session Delivery Overview**\n\n## **Introduction**\n\nThis document provides an overview of the training performance across **2023–2025**. It aims to analyse the number of sessions delivered during this period to understand the overall demand for each course. The analysis supports data-driven planning for future training programs and helps identify areas with higher participation or emerging needs. This aligns with the organisation’s goal to improve training effectiveness and resource allocation.\n\n## **Objectives**\n\nThe key objectives of this documentation are to:\n\n* Determine the total number of sessions delivered per course during **2023–2025**.\n* Identify trends in session frequency across different training categories.\n* Highlight areas of growth or decline to guide future program planning.\n* Support decisions on course scheduling, instructor allocation, and resource investment.\n\nSuccess will be measured by the clarity and accuracy of insights drawn from the data, enabling actionable planning for the next training cycle.\n\n## **Scope**\n\nThe analysis covers all training sessions delivered between **January 2023 and December 2025**. It includes course titles, frequency, and participation data where available. The scope is limited to quantitative delivery metrics and does not include qualitative feedback or performance evaluation. All data is sourced from the internal training records and validated for consistency.\n\n## **Methodology**\n\nThe analysis follows these key steps:\n\n1. Collect and verify session data from the **2023–2025** training records.\n2. Organise courses by category and delivery frequency.\n3. Apply filters to identify patterns and variations in demand.\n4. Summarise findings in tables and charts for clarity.\n\nThis structured approach ensures that the data is complete, accurate, and ready for interpretation.\n\n## **Expected Outcomes**\n\nThe expected outcomes of this analysis include:\n\n* A clear summary of session delivery volume for each course.\n* Insights into training demand trends across the **three-year period**.\n* Recommendations for strategic adjustments in future training plans.\n\nThe results will guide planning for the **2026 training calendar**, ensuring efficient use of resources and alignment with learner demand.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport warnings\n\n# Suppress \"serial value outside limits\" warnings\nwith warnings.catch_warnings(record=True):\n    warnings.simplefilter(\"ignore\")\n\n    # Read Excel without date parsing\n    data = pd.read_excel(\n        \"/kaggle/input/trainingdata/TKI-Course-repository.xlsx\",\n        engine=\"openpyxl\",\n        parse_dates=False,  # ✅ correct parameter\n        dtype=str            # ✅ read everything as text to avoid Excel date conversion\n    )\n\n# Check Data Structure\nprint(\"Data Structure:\")\nprint(data.info())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:06.242724Z","iopub.execute_input":"2025-11-05T09:12:06.242991Z","iopub.status.idle":"2025-11-05T09:12:39.805543Z","shell.execute_reply.started":"2025-11-05T09:12:06.242964Z","shell.execute_reply":"2025-11-05T09:12:39.804437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Data Preparation**\n\nThis step creates a structured dataset focused on analyzing course sessions delivered between **2023 and 2025**. The process involves selecting relevant fields, standardizing date formats, and filtering by year to ensure clean and consistent data for analysis.\n\n- **Column Selection:**  \n  From the original dataset, nine key columns are retained to capture essential course and scheduling details:  \n  **Organizer**, **Facilitator**, **Course Title**, **Course Structure**, **Method**, **Start Date**, **End Date**, **Course City**, and **Course Country**.  \n  These columns provide complete coverage of delivery, timing, and location aspects, removing unrelated fields to improve efficiency and clarity.\n\n- **Date Standardization:**  \n  A robust multi-pass date parser converts all date values into a consistent format.  \n  It handles:\n  - Excel serial numbers (e.g., `45250` → valid date).  \n  - Day-first formats (`28.09.2025`, `28/09/2025`, `28-09-2025`).  \n  - Year-first ISO formats (`2025-09-28`, `2025/09/28`).  \n  - Month-first fallbacks (`09/28/2025`).  \n  The cleaned results are stored in two new columns — **`Start date std`** and **`End date std`**.\n\n- **Filtering by Year:**  \n  The dataset is filtered to include only sessions where either the start or end date falls within **2023, 2024, or 2025**.  \n  Any sessions with dates from **2022** are explicitly removed to keep the analysis focused on recent delivery activity.\n\n- **Verification Output:**  \n  The script prints the total number of valid rows and displays the first and last 20 records to confirm accuracy after filtering and parsing.","metadata":{"execution":{"iopub.status.busy":"2025-10-09T09:43:33.219988Z","iopub.execute_input":"2025-10-09T09:43:33.221131Z","iopub.status.idle":"2025-10-09T09:43:35.807324Z","shell.execute_reply.started":"2025-10-09T09:43:33.221089Z","shell.execute_reply":"2025-10-09T09:43:35.806417Z"}}},{"cell_type":"code","source":"# Use the existing dataframe named `data`\n# Keep ONLY these columns\nselected_columns = [\n    'Organizer',\n    'Facilitator',\n    'Course title',\n    'Course structure',\n    'Method',\n    'Start date',\n    'End date',\n    'Course city',\n    'Course country',\n]\n\n# Create the working dataset named `course`\ncourse = data[selected_columns].copy()\n\n# ---------- Robust multi-pass date parser (handles '.', '/', '-' and Excel serials) ----------\ndef parse_dates(s: pd.Series) -> pd.Series:\n    # Clean once\n    s = s.astype('string').str.strip()\n\n    # 1) Excel serials first (accept '45250' and '45250.0'; avoid short numbers like '2025')\n    ser_mask = s.str.fullmatch(r'\\d{5,}(?:\\.0+)?')\n    ser = pd.to_numeric(s.where(ser_mask).str.replace(r'\\.0+$', '', regex=True), errors='coerce')\n    out = pd.to_datetime(ser, unit='d', origin='1899-12-30', errors='coerce')\n\n    # 2) Day-first strings: '28.09.2025', '28/9/2025', '28-09-2025'\n    m = out.isna()\n    out[m] = pd.to_datetime(s[m], dayfirst=True, errors='coerce', cache=True)\n\n    # 3) Year-first (ISO etc.): '2025-09-28', '2025/09/28'\n    m = out.isna()\n    out[m] = pd.to_datetime(s[m], yearfirst=True, errors='coerce', cache=True)\n\n    # 4) Last-chance month-first: '09/28/2025'\n    m = out.isna()\n    out[m] = pd.to_datetime(s[m], errors='coerce', cache=True)\n\n    return out\n# --------------------------------------------------------------------------------------------\n\n# Create standardized datetime columns\ncourse['Start date std'] = parse_dates(course['Start date'])\ncourse['End date std']   = parse_dates(course['End date'])\n\n# Keep ONLY rows where Start date std OR End date std is in 2023/2024/2025\nvalid_years = {2023, 2024, 2025}\nmask_years = (\n    course['Start date std'].dt.year.isin(valid_years) |\n    course['End date std'].dt.year.isin(valid_years)\n)\ncourse = course[mask_years].reset_index(drop=True)\n# Explicitly exclude any row where Start/End std is in 2022\ncourse = course[~(course['Start date std'].dt.year.eq(2022) | course['End date std'].dt.year.eq(2022))].reset_index(drop=True)\n\n# (Optional) quick look\nprint(\"Filtered rows (years 2023–2025 in Start/End std):\", len(course))\ndisplay(course.head(20))\ndisplay(course.tail(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:39.807084Z","iopub.execute_input":"2025-11-05T09:12:39.807512Z","iopub.status.idle":"2025-11-05T09:12:40.042521Z","shell.execute_reply.started":"2025-11-05T09:12:39.807488Z","shell.execute_reply":"2025-11-05T09:12:40.041858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Duplicate Check and Participant Count\n\nIn this step, we define one session and count how many participant records belong to it. This helps us measure session volume without changing the source data.\n\n#### Session Rule\nA session is the unique combination of four columns: **Facilitator**, **Course Title**, **Method**, **Start Date**, and **End Date**.\n\n#### What We Do\n1. Find duplicate rows based on the four columns above.  \n2. Add a new column **Participants Count**, which stores the size of each group defined by the four columns.  \n3. For any row with no duplicate, set **Participants Count** to 1. This column has no missing values.  \n4. Produce three checks for quick validation:  \n   - **Total rows**: the number of records in the filtered dataset.  \n   - **Total participants count**: the sum of *Participants Count* across unique sessions. This equals the number of rows.  \n   - **Total sessions**: the number of unique combinations of the four key columns.  \n5. Show a short preview so we can confirm the result.\n\n#### Assumptions and Guardrails\n- We do not change the data type of any existing column.  \n- We only add the **Participants Count** column.  \n- All four grouping columns must be present and readable in the dataset.","metadata":{}},{"cell_type":"code","source":"# Define the session key\nkey_cols = ['Facilitator', 'Course title', 'Start date', 'End date']\n\n# Count rows per unique session, aligned back to each row\ngroup_sizes = course.groupby(key_cols, dropna=False).size()\ncourse['Participants count'] = course[key_cols].merge(\n    group_sizes.rename('Participants count').reset_index(),\n    on=key_cols,\n    how='left'\n)['Participants count']\n\n# Safety check: ensure no missing values in the new column, default to 1 if any appear\ncourse['Participants count'] = course['Participants count'].fillna(1).astype(int)\n\n# Summary numbers\ntotal_rows = len(course)\ntotal_sessions = group_sizes.shape[0]\ntotal_participants = int(group_sizes.sum())  # equals total_rows by construction\n\n# Print summaries\nprint(\"Total rows:\", total_rows)\nprint(\"Total participants count:\", total_participants)\nprint(\"Total sessions:\", total_sessions)\n\n# Preview\nprint(\"\\nPreview:\")\ndisplay(course.head(20))   # top 20\ndisplay(course.tail(20))   # bottom 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.043272Z","iopub.execute_input":"2025-11-05T09:12:40.043575Z","iopub.status.idle":"2025-11-05T09:12:40.099232Z","shell.execute_reply.started":"2025-11-05T09:12:40.043548Z","shell.execute_reply":"2025-11-05T09:12:40.098379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Remove duplicates, keep one row per session\n\nWe will drop duplicate rows using the session keys, Facilitator, Course title, Start date, End date. We keep the first row of each session. The Participants count column already stores the group size, so the kept row holds the correct count. No other column types change.","metadata":{}},{"cell_type":"code","source":"# Keys that define one session\nkey_cols = ['Facilitator', 'Course title', 'Start date', 'End date']\n\n# Remove duplicates, keep the first row per session\ncourse_dedup = course.drop_duplicates(subset=key_cols, keep='first').copy()\n\n# Quick checks\nprint(\"Rows before:\", len(course))\nprint(\"Rows after :\", len(course_dedup))\nprint(\"Rows removed:\", len(course) - len(course_dedup))\n\nprint(\"\\nPreview after dedupe:\")\ndisplay(course_dedup.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.101234Z","iopub.execute_input":"2025-11-05T09:12:40.101496Z","iopub.status.idle":"2025-11-05T09:12:40.124805Z","shell.execute_reply.started":"2025-11-05T09:12:40.101476Z","shell.execute_reply":"2025-11-05T09:12:40.124084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if there are still duplicates after the cleaning process\nremaining_duplicates = course_dedup[course_dedup.duplicated(subset=['Facilitator', 'Course title', 'Start date', 'End date'], keep=False)]\n\n# Display the results\nif len(remaining_duplicates) == 0:\n    print(\"✅ No duplicates remain. The data is clean.\")\nelse:\n    print(f\"⚠️ There are still {len(remaining_duplicates)} duplicates remaining.\")\n    display(remaining_duplicates.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.125569Z","iopub.execute_input":"2025-11-05T09:12:40.125781Z","iopub.status.idle":"2025-11-05T09:12:40.133747Z","shell.execute_reply.started":"2025-11-05T09:12:40.125763Z","shell.execute_reply":"2025-11-05T09:12:40.133002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(course_dedup.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.134555Z","iopub.execute_input":"2025-11-05T09:12:40.134795Z","iopub.status.idle":"2025-11-05T09:12:40.157061Z","shell.execute_reply.started":"2025-11-05T09:12:40.134776Z","shell.execute_reply":"2025-11-05T09:12:40.156155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure the table to scroll\ndef table(df, height=400):\n    display(df.style.set_table_attributes(f'style=\"display:inline-block;overflow:auto;height:{height}px;width:auto;\"').set_table_styles([{\n        'selector': '',\n        'props': [('border-collapse', 'collapse'),\n                  ('margin', '0px')]}]))\n\n# Display the table\ntable(course_dedup)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.158014Z","iopub.execute_input":"2025-11-05T09:12:40.158302Z","iopub.status.idle":"2025-11-05T09:12:40.572635Z","shell.execute_reply.started":"2025-11-05T09:12:40.158279Z","shell.execute_reply":"2025-11-05T09:12:40.57178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export cleaned data to Excel file\noutput_path = '/kaggle/working/Course-Season-2023-2025-Participants.xlsx'\ncourse_dedup.to_excel(output_path, index=False)\n\nprint(f\"✅ Data successfully exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:40.57354Z","iopub.execute_input":"2025-11-05T09:12:40.573893Z","iopub.status.idle":"2025-11-05T09:12:40.96349Z","shell.execute_reply.started":"2025-11-05T09:12:40.57387Z","shell.execute_reply":"2025-11-05T09:12:40.962302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Duration & Quality Checks\n\n**What this step does (using `course_dedup`):**\n\n1. **Duration math (inclusive):**\n   Creates `Duration (days)` as\n   `End date std − Start date std + 1`,\n   so a one-day course counts as **1** (not 0).\n\n2. **Targeted flags (`suspicious_mask`):**\n   Marks rows for review when:\n\n   * `Duration (days) < 1` → end before start (likely parse/data issue) and same date, or\n   * `Duration (days) > 15` → unusually long course.\n\n3. **Clear outputs for review:**\n\n   * **Rows with suspicious durations:** shows key columns plus `Start date std`, `End date std`, and `Duration (days)`.\n   * **Rows with unparsed dates (NaT):** any row where either `Start date std` or `End date std` is missing.\n\n4. **Summary counters:**\n   Prints total rows in `course_dedup`, count of **NaT** rows, and count of **suspicious** rows.","metadata":{}},{"cell_type":"code","source":"# ---- NEXT STEP: duration math, flags, and review tables (use course_dedup) ----\n# Assumes `course_dedup` already has parsed columns: 'Start date std' and 'End date std'\n\n# 1) Duration (inclusive) — a one-day course = 1 day\ncourse_dedup['Duration (days)'] = (\n    course_dedup['End date std'] - course_dedup['Start date std']\n).dt.days + 1\n\n# 2) Targeted flags\n#    - Duration < 2  : end before start (likely parsing/data error) and same date check\n#    - Duration > 15 : unusually long, needs review\nsuspicious_mask = (course_dedup['Duration (days)'] < 2) | (course_dedup['Duration (days)'] > 15)\n\n# 3) Clear outputs\n#    a) Rows with unparsed dates (either std date is NaT)\nnat_mask = course_dedup['Start date std'].isna() | course_dedup['End date std'].isna()\nnat_table = course_dedup.loc[nat_mask, [\n    'Organizer', 'Facilitator', 'Course title', 'Course structure', 'Method', 'Participants count',\n    'Start date', 'End date', 'Course city', 'Course country'\n]]\n\n#    b) Rows with suspicious durations\nsuspicious_table = course_dedup.loc[suspicious_mask, [\n    'Organizer', 'Facilitator', 'Course title', 'Course structure', 'Method', 'Participants count',\n    'Start date', 'End date', 'Start date std', 'End date std',\n    'Duration (days)', 'Course city', 'Course country'\n]]\n\n# 4) Summary + displays\nrows_total = len(course_dedup)\nnat_count = nat_mask.sum()\nsuspicious_count = suspicious_mask.sum()\n\nprint(f\"Total rows (dedup): {rows_total}\")\nprint(f\"Rows with unparsed dates (NaT): {nat_count}\")\nprint(f\"Rows with suspicious durations (<1 or >15 days): {suspicious_count}\\n\")\n\nprint(\" ROWS WITH SUSPICIOUS DURATION \")\ndisplay(suspicious_table)\n\nprint(\"\\n ROWS THAT FAILED TO PARSE (NaT) \")\ndisplay(nat_table)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:12:45.776756Z","iopub.execute_input":"2025-11-05T09:12:45.777309Z","iopub.status.idle":"2025-11-05T09:12:45.812689Z","shell.execute_reply.started":"2025-11-05T09:12:45.777284Z","shell.execute_reply":"2025-11-05T09:12:45.811719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Data Cleaning & Validation Plan**\n\n#### **1. Identify Ambiguity**  \nFor each date in the `suspicious_table`, check if both the **day** and **month** fall between 1 and 12.  \nThis helps detect potentially ambiguous dates that may have been misinterpreted due to mixed formats (e.g., `03/07` could mean **March 7** or **July 3**).\n\n#### **2. Create a Flag**  \nAdd a new column that **flags rows with ambiguous dates**.  \nThis makes it easy to isolate and review problematic entries before applying corrections.\n\n#### **3. Swap Day and Month**  \nFor all **flagged dates**, swap the **day** and **month** values.  \nThis corrects possible day/month parsing errors caused by differing date conventions (e.g., **US vs. EU** formats).\n\n#### **4. Recalculate Duration**  \nAfter correcting dates, **recalculate the duration** (e.g., between `start_date` and `end_date`) to ensure logical and consistent time intervals.\n\n#### **5. Show Before & After**  \nProduce a **comparison table** displaying both versions:\n- Original (ambiguous) data  \n- Corrected (cleaned) data  \n\nThis visual check verifies the accuracy of the cleaning process before finalizing the dataset.","metadata":{"execution":{"iopub.status.busy":"2025-10-20T02:43:54.284009Z","iopub.execute_input":"2025-10-20T02:43:54.284296Z","iopub.status.idle":"2025-10-20T02:43:54.331746Z","shell.execute_reply.started":"2025-10-20T02:43:54.284276Z","shell.execute_reply":"2025-10-20T02:43:54.330887Z"}}},{"cell_type":"code","source":"# Create a copy to work on, preserving the original table\nresolved_table = suspicious_table.copy()\n\nprint(\" Original Suspicious Table \")\nprint(\"Notice 'Course B' has a negative duration, indicating a parsing error.\")\ndisplay(suspicious_table)\n\n\n# 1. Define a function to check if a date is ambiguous (day and month are 1-12)\ndef is_date_ambiguous(dt):\n    \"\"\"Returns True if a datetime object's day and month are both between 1 and 12.\"\"\"\n    if pd.isna(dt):\n        return False\n    return 1 <= dt.day <= 12 and 1 <= dt.month <= 12\n\n# 2. Apply this function to create boolean flags for start and end dates\nresolved_table['start_is_ambiguous'] = resolved_table['Start date std'].apply(is_date_ambiguous)\nresolved_table['end_is_ambiguous'] = resolved_table['End date std'].apply(is_date_ambiguous)\n\n# Create a single flag for the entire row\nresolved_table['Ambiguous Flag'] = resolved_table['start_is_ambiguous'] | resolved_table['end_is_ambiguous']\n\n\n# 3. Define a function to swap the day and month of a datetime object\ndef swap_day_month(dt):\n    \"\"\"Returns a new datetime object with the day and month swapped.\"\"\"\n    if pd.isna(dt):\n        return pd.NaT\n    # Create a new Timestamp to avoid errors with non-existent dates (e.g., 31st of Feb)\n    return pd.Timestamp(year=dt.year, month=dt.day, day=dt.month)\n\n\n# 4. Create new 'resolved' columns\n# Initialize them with the original dates\nresolved_table['Start date resolved'] = resolved_table['Start date std']\nresolved_table['End date resolved'] = resolved_table['End date std']\n\n# Conditionally apply the swap only where the dates were flagged as ambiguous\nresolved_table.loc[resolved_table['start_is_ambiguous'], 'Start date resolved'] = resolved_table.loc[resolved_table['start_is_ambiguous'], 'Start date std'].apply(swap_day_month)\nresolved_table.loc[resolved_table['end_is_ambiguous'], 'End date resolved'] = resolved_table.loc[resolved_table['end_is_ambiguous'], 'End date std'].apply(swap_day_month)\n\n\n# 5. Recalculate the duration based on the new 'resolved' dates\nnew_duration = (resolved_table['End date resolved'] - resolved_table['Start date resolved'])\nresolved_table['Duration resolved (days)'] = new_duration.dt.days + 1\n\n\n# 6. Create the final \"Before and After\" comparison table for easy review\ncomparison_view = resolved_table[[\n    'Course title',\n    'Start date std',\n    'End date std',\n    'Duration (days)',\n    'Ambiguous Flag',\n    'Start date resolved',\n    'End date resolved',\n    'Duration resolved (days)'\n]].copy()\n\n# Format dates for clearer display\nfor col in ['Start date std', 'End date std', 'Start date resolved', 'End date resolved']:\n    comparison_view[col] = comparison_view[col].dt.strftime('%Y-%m-%d')\n\n\nprint(\"\\n\\n Corrected Dates and Durations (Comparison View) \")\nprint(\"The 'resolved' columns show the corrected dates and logical durations.\")\ndisplay(comparison_view)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:13:19.605205Z","iopub.execute_input":"2025-11-05T09:13:19.605558Z","iopub.status.idle":"2025-11-05T09:13:19.659186Z","shell.execute_reply.started":"2025-11-05T09:13:19.605534Z","shell.execute_reply":"2025-11-05T09:13:19.65853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Course Duration Quality Checks**\n\n#### **1. Unusually Long Courses**  \nIdentify all courses lasting **more than 15 days**.  \nThese may suggest data entry mistakes or incorrect end dates (e.g., an extra zero inflating the duration).\n\n#### **2. Illogical Duration**  \nFlag any courses with durations **less than 1 day**.  \nSuch cases often mean the **end date is earlier than the start date**, indicating a possible reversal or input error.\n\n#### **3. Failed to Parse (NaT)**  \nDetect rows where **date parsing failed**, resulting in `NaT` (*Not a Time*) values.  \nThese records represent **invalid or unreadable date formats** that require manual review or correction.","metadata":{}},{"cell_type":"code","source":"# Allow full row display\npd.set_option('display.max_rows', None)\n\n#  Main Logic: Filter for remaining problematic rows \n\n# 1. Filter for courses with unusually long durations\nlong_duration_mask = resolved_table['Duration resolved (days)'] > 15\nlong_duration_table = resolved_table[long_duration_mask]\n\n# 2. Filter for courses with illogical durations (less than 1 day and same date check)\nshort_duration_mask = resolved_table['Duration resolved (days)'] < 2\nshort_duration_table = resolved_table[short_duration_mask]\n\n# 3. Define columns for a clean, consistent view\nreview_cols = [\n    'Organizer', 'Course title','Facilitator', 'Participants count',\n    'Start date std', 'End date std',\n    'Start date resolved', 'End date resolved', 'Duration resolved (days)'\n]\n\n#  Display Final Actionable Tables \n\nprint(\"FINAL REVIEW LISTS\")\nprint(\"Use these tables to perform manual corrections on your source data.\")\n\nprint(\"\\n\\n## 1. Unusually Long Duration (> 15 days)\")\nprint(\"Check if these long durations are correct or if there's a typo in the year or month.\")\nif long_duration_table.empty:\n    print(\"--> No courses found with a duration longer than 15 days.\")\nelse:\n    display(long_duration_table[review_cols])\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n\nprint(\"## 2. Illogical Duration (< 1 day)\")\nprint(\"The End Date is before the Start Date. This requires manual correction.\")\nif short_duration_table.empty:\n    print(\"--> No courses found with a negative duration.\")\nelse:\n    display(short_duration_table[review_cols])\n    \nprint(\"\\n\" + \"=\"*50 + \"\\n\") # Separator\n\nprint(\"## 3. Failed to Parse (NaT Rows)\")\nprint(\"These rows contain text or formats that could not be converted to dates.\")\nif nat_table.empty:\n    print(\"--> No rows failed to parse.\")\nelse:\n    display(nat_table)","metadata":{"execution":{"iopub.status.busy":"2025-11-05T09:27:50.809674Z","iopub.execute_input":"2025-11-05T09:27:50.809991Z","iopub.status.idle":"2025-11-05T09:27:50.86007Z","shell.execute_reply.started":"2025-11-05T09:27:50.809966Z","shell.execute_reply":"2025-11-05T09:27:50.859211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0) Bootstrap working copy from previous step (NO re-parsing)\n#    Assumes course_dedup already has 'Start date std', 'End date std', and (optionally) 'Duration (days)'\ncourse_clean = course_dedup.copy()\n\n#  1. Consolidate Automatic Fixes into the Main DataFrame \n# First, let's create the 'resolved' columns in your main 'course_clean' DataFrame.\n# We'll start by copying the standard parsed dates.\ncourse_clean['Start date resolved'] = course_clean['Start date std']\ncourse_clean['End date resolved'] = course_clean['End date std']\ncourse_clean['Duration resolved (days)'] = course_clean['Duration (days)']\n\n# Now, update 'course_clean' with the automatic swaps you performed in 'resolved_table'.\n# The .update() method neatly transfers the corrected data back to your main table.\ncourse_clean.update(resolved_table)\n\n\n#  2. Manual Input: Define your corrections here \n# Use the DataFrame index as the key.\n# Provide the CORRECT 'start' and 'end' dates in 'YYYY-MM-DD' format.\nmanual_corrections = {\n     # Long Duration (> 15 days)\n  775: {'start': '2023-02-21', 'end': '2023-03-21'}, # correct\n  6665: {'start': '2024-02-05', 'end': '2024-03-09'}, #swapped month in start and end\n  10682: {'start': '2024-10-04', 'end': '2024-11-01'}, # correct\n  11849: {'start': '2024-11-10', 'end': '2024-11-12'}, #swapped month in end\n  13185: {'start': '2025-01-06', 'end': '2025-01-10'}, #swapped month in end\n  13625: {'start': '2025-02-03', 'end': '2025-02-07'}, #swapped month in end\n  13823: {'start': '2025-02-09', 'end': '2025-02-09'}, #swapped month in end\n  13839: {'start': '2025-02-10', 'end': '2025-02-10'}, #swapped month in end\n  14073: {'start': '2025-03-02', 'end': '2025-03-06'}, #swapped month in end\n  14314: {'start': '2025-04-07', 'end': '2025-04-11'}, #swapped month in end\n  14327: {'start': '2025-04-06', 'end': '2025-04-10'}, #swapped month in end\n  14823: {'start': '2025-05-04', 'end': '2025-05-08'}, #swapped month in end\n  14837: {'start': '2025-05-04', 'end': '2025-05-08'}, #swapped month in end\n  14848: {'start': '2025-05-04', 'end': '2025-05-08'}, #swapped month in end\n  14856: {'start': '2025-05-04', 'end': '2025-05-08'}, #swapped month in end\n  14917: {'start': '2025-05-04', 'end': '2025-05-08'}, #swapped month in end\n  15931: {'start': '2025-07-06', 'end': '2025-07-10'}, #swapped month in end\n  15948: {'start': '2025-07-06', 'end': '2025-07-08'}, #swapped month in end\n  15968: {'start': '2025-07-06', 'end': '2025-07-10'}, #swapped month in end\n  16496: {'start': '2025-08-10', 'end': '2025-08-12'}, #swapped month in end\n  16522: {'start': '2025-08-10', 'end': '2025-08-12'}, #swapped month in end\n  16541: {'start': '2025-08-17', 'end': '2025-08-19'},  # End year was likely 2025, not 2028 and month in end typo, should be 08\n  16906: {'start': '2025-09-01', 'end': '2025-09-10'}, #swapped month in end\n  16985: {'start': '2025-09-07', 'end': '2025-09-11'}, #swapped month in end\n  17011: {'start': '2025-09-08', 'end': '2025-09-12'}, #swapped month in end\n  17045: {'start': '2025-09-08', 'end': '2025-09-12'}, #swapped month in end\n  17056: {'start': '2025-09-08', 'end': '2025-09-12'}, #swapped month in end\n  17085: {'start': '2025-09-07', 'end': '2025-09-11'}, #swapped month in end\n  17091: {'start': '2025-09-07', 'end': '2025-09-11'}, #swapped month in end\n    \n    \n    # Illogical Duration (less than 1 day) and same date check\n    1936: {'start': '2023-05-03', 'end': '2023-05-21'}, # date swapped beetween start and end\n    7577: {'start': '2024-04-22', 'end': '2024-04-26'}, # year is wrong in start\n    10793: {'start': '2024-10-07', 'end': '2024-10-10'}, # year is wrong in start and month swapped in start\n    11690: {'start': '2024-11-05', 'end': '2024-11-07'}, #swapped month in end\n    11714: {'start': '2024-11-03', 'end': '2024-11-07'}, #swapped month in end\n    12428: {'start': '2024-12-01', 'end': '2024-12-05'}, #swapped month in end\n    12477: {'start': '2024-12-02', 'end': '2024-12-05'}, #swapped month in end\n    12532: {'start': '2024-12-01', 'end': '2024-12-05'}, #swapped month in end\n    14978: {'start': '2025-05-11', 'end': '2025-05-13'}, #month in end typo\n    15370: {'start': '2025-06-01', 'end': '2025-06-04'}, #swapped month in end\n    15384: {'start': '2025-06-02', 'end': '2025-06-03'}, #swapped month in end\n    15746: {'start': '2025-06-24', 'end': '2025-06-25'},  # date swapped beetween start and end\n    16112: {'start': '2025-07-15', 'end': '2025-07-17'}, #month in end typo\n    16234: {'start': '2025-07-20', 'end': '2025-07-27'},  # date swapped beetween start and end\n    16381: {'start': '2025-08-03', 'end': '2025-08-07'}, #month in end typo\n\n    # NaT Rows (Failed to Parse)\n    6921:    {'start': '2025-02-25', 'end': '2025-02-28'},  # 29/02/2025 is invalid, corrected to 28\n    15544: {'start': '2025-06-22', 'end': '2025-06-26'},  # Correcting typo in end date year (20225 -> 2025)\n}\n\n\n#  3. Apply Corrections and Recalculate \n\n# Get the list of rows we are about to fix for the \"Before\" view\nindices_to_fix = list(manual_corrections.keys())\nprint(\" DATA BEFORE MANUAL FIX \")\n# Use course_clean, your actual DataFrame name\ndisplay(course_clean.loc[indices_to_fix])\n\n# Loop through the dictionary and apply each correction to your main DataFrame\nfor index, dates in manual_corrections.items():\n    if index in course_clean.index:\n        # Convert string dates to datetime objects\n        start_date = pd.to_datetime(dates['start'], errors='coerce')\n        end_date = pd.to_datetime(dates['end'], errors='coerce')\n\n        # Assign the corrected dates\n        course_clean.loc[index, 'Start date resolved'] = start_date\n        course_clean.loc[index, 'End date resolved'] = end_date\n        \n        # Recalculate duration for the fixed row\n        duration = (end_date - start_date).days + 1\n        course_clean.loc[index, 'Duration resolved (days)'] = duration\n    else:\n        print(f\"Warning: Index {index} not found in DataFrame.\")\n\n# Ensure integer dtype (nullable) for the resolved duration column\ncourse_clean['Duration resolved (days)'] = course_clean['Duration resolved (days)'].astype('Int64')\n\nprint(\"\\n\\n DATA AFTER MANUAL FIX \")\nprint(\"Durations are now logical and NaT rows have been populated. ✅\")\n# Display the same rows from course_clean to see the \"After\" view\ndisplay(course_clean.loc[indices_to_fix])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:06.57847Z","iopub.execute_input":"2025-11-05T09:30:06.578743Z","iopub.status.idle":"2025-11-05T09:30:06.786468Z","shell.execute_reply.started":"2025-11-05T09:30:06.578723Z","shell.execute_reply":"2025-11-05T09:30:06.785687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export cleaned data to Excel file\noutput_path = '/kaggle/working/Course-Season-2023-2025-Clean.xlsx'\ncourse_clean.to_excel(output_path, index=False)\n\nprint(f\"✅ Data successfully exported to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:16.757169Z","iopub.execute_input":"2025-11-05T09:30:16.757514Z","iopub.status.idle":"2025-11-05T09:30:17.100892Z","shell.execute_reply.started":"2025-11-05T09:30:16.757488Z","shell.execute_reply":"2025-11-05T09:30:17.100121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure the table to scroll\ndef table(df, height=400):\n    display(df.style.set_table_attributes(f'style=\"display:inline-block;overflow:auto;height:{height}px;width:auto;\"').set_table_styles([{\n        'selector': '',\n        'props': [('border-collapse', 'collapse'),\n                  ('margin', '0px')]}]))\n\n# Display the table\ntable(course_clean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:21.964011Z","iopub.execute_input":"2025-11-05T09:30:21.964274Z","iopub.status.idle":"2025-11-05T09:30:22.391301Z","shell.execute_reply.started":"2025-11-05T09:30:21.964254Z","shell.execute_reply":"2025-11-05T09:30:22.390373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(course_clean.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:42.359275Z","iopub.execute_input":"2025-11-05T09:30:42.359981Z","iopub.status.idle":"2025-11-05T09:30:42.373194Z","shell.execute_reply.started":"2025-11-05T09:30:42.359952Z","shell.execute_reply":"2025-11-05T09:30:42.372233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Add Year Column and Compute Yearly Training Statistics\n\nThe following code performs several key steps to analyze training performance data by year:\n\n1. **Adds a `Year` column**  \n   - Uses `Start date resolved` if available.  \n   - Falls back to `End date resolved` if `Start date` is missing.\n\n2. **Computes yearly aggregates**  \n   - Total participants per year.  \n   - Total number of courses per year.","metadata":{}},{"cell_type":"code","source":"# Adds a Year column (uses Start date resolved; if missing, falls back to End date resolved).\n# Computes total participants per year.\n# Computes number of courses per year.\n\n# 1) Add 'Year' column: prefer Start date resolved, else use End date resolved\nyear_from_start = course_clean['Start date resolved'].dt.year\nyear_from_end = course_clean['End date resolved'].dt.year\n\n# Use start year where available; otherwise take end year. Keep pandas nullable integer dtype.\ncourse_clean['Year'] = year_from_start.where(year_from_start.notna(), year_from_end).astype('Int64')\n\n# 2) Aggregate by year\n#    - total participants per year\n#    - total number of courses per year\nyearly_stats = (\n    course_clean.dropna(subset=['Year'])  # exclude rows where both dates were missing\n    .groupby('Year', dropna=False)\n    .agg(\n        total_participants=('Participants count', 'sum'),\n        total_courses=('Year', 'size')\n    )\n    .reset_index()\n    .sort_values('Year')\n)\n\n# If you’re in a notebook and want to see it nicely:\ndisplay(yearly_stats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:45.388457Z","iopub.execute_input":"2025-11-05T09:30:45.389267Z","iopub.status.idle":"2025-11-05T09:30:45.417931Z","shell.execute_reply.started":"2025-11-05T09:30:45.38924Z","shell.execute_reply":"2025-11-05T09:30:45.417214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cohorts & Participants by Method\n\n**Purpose:** Summarize per **Year × Course title × Method**.\n\n**Requires:** `course_clean` with `Year`, `Course title`, `Method`, `Participants count`.\n\n**Logic:**\n\n* Keep methods: **Face to face**, **Live**; drop missing `Year`.\n* Group by Year/Course/Method → `cohorts` (row count), `participants` (sum).\n* Pivot wide to columns:\n  `Cohorts (Face to face)`, `Cohorts (Live)`,\n  `Participants (Face to face)`, `Participants (Live)`; fill missing with 0.\n* Sort and produce `per_year_combined`.\n\n**Outputs:**\n\n* `per_year_combined` (all years).\n* `tables_by_year[YYYY]` (per year).\n* Excel: `per_year_course_cohorts_participants.xlsx` (one sheet per year).\n\n**Notes:** Only the two methods are included; rows without `Year` are excluded.","metadata":{}},{"cell_type":"code","source":"# === Per-year, per-course cohorts & participants by delivery method ===\n# Definition:\n# - A \"cohort\" = how many times a course was delivered in a given calendar year.\n#   Since each row in `course_clean` is one delivery/session, cohorts are the\n#   number of rows per (Year, Course title, Method).\n# - \"Face to face\" and \"Live\" come from the `Method` column (categorical, fixed values).\n# - Year comes from your existing 'Year' column (derived from Start date resolved, else End date resolved).\n\nfrom IPython.display import display\n\n# Work on a copy to avoid mutating the original\ndf = course_clean.copy(deep=True)\n\n# Keep only the two valid delivery methods (as per your note)\nvalid_methods = [\"Face to face\", \"Live\"]\ndf = df[df[\"Method\"].isin(valid_methods)].copy()\n\n# Guard: drop rows where Year is missing (means both dates were missing)\ndf = df.dropna(subset=[\"Year\"])\n\n# 1) Group to compute cohorts and participants per (Year, Course title, Method)\n#    - cohorts = number of deliveries (row count)\n#    - participants = sum of Participants count\nby_ycm = (\n    df.groupby([\"Year\", \"Course title\", \"Method\"], as_index=False)\n      .agg(\n          cohorts=(\"Course title\", \"size\"),\n          participants=(\"Participants count\", \"sum\")\n      )\n)\n\n# 2) Pivot wider so each Method becomes its own pair of columns\nwide = (\n    by_ycm.pivot_table(\n        index=[\"Year\", \"Course title\"],\n        columns=\"Method\",\n        values=[\"cohorts\", \"participants\"],\n        aggfunc=\"sum\",\n        fill_value=0\n    )\n)\n\n# 3) Flatten MultiIndex columns and title them exactly as requested\nwide.columns = [\n    f\"{('Cohorts' if m=='cohorts' else 'Participants')} ({meth})\"\n    for (m, meth) in wide.columns\n]\nwide = wide.reset_index().sort_values([\"Year\", \"Course title\"])\n\n# 4) Ensure all expected columns exist even if a method is missing for some years\nexpected_cols = [\n    \"Cohorts (Face to face)\", \"Cohorts (Live)\",\n    \"Participants (Face to face)\", \"Participants (Live)\"\n]\nfor col in expected_cols:\n    if col not in wide.columns:\n        wide[col] = 0\n\n# 5) Final combined table (all years)\nper_year_combined = wide[\n    [\"Year\", \"Course title\"]\n    + expected_cols\n].copy()\n\n# Show the combined table (nicely in notebooks)\ndisplay(per_year_combined)\n\n# 6) Split into separate tables per year (drop the Year column inside each sheet)\ntables_by_year = {\n    int(year): sub.drop(columns=\"Year\").reset_index(drop=True)\n    for year, sub in per_year_combined.groupby(\"Year\", dropna=False)\n}\n\n# Example: access a specific year's table\n# display(tables_by_year[2023].head())\n\n# 7) Write to Excel with one sheet per year (sheet name = the year)\n#    If you only want exactly 3 sheets, see the commented block below.\nout_path = \"per_year_course_cohorts_participants.xlsx\"\nwith pd.ExcelWriter(out_path) as writer:\n    for year in sorted(tables_by_year):\n        sheet_name = f\"{year}\"\n        tables_by_year[year].to_excel(writer, sheet_name=sheet_name, index=False)\n\nprint(f\"Excel saved to: {out_path}\")\n\n# --- If you specifically want only 3 sheets (e.g., first three chronological years), use this instead:\n# first_three_years = sorted(tables_by_year)[:3]\n# out_path_3 = \"per_year_course_cohorts_participants_first3.xlsx\"\n# with pd.ExcelWriter(out_path_3) as writer:\n#     for year in first_three_years:\n#         tables_by_year[year].to_excel(writer, sheet_name=str(year), index=False)\n# print(f\"Excel (first 3 years) saved to: {out_path_3}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:30:50.814731Z","iopub.execute_input":"2025-11-05T09:30:50.815526Z","iopub.status.idle":"2025-11-05T09:30:50.888382Z","shell.execute_reply.started":"2025-11-05T09:30:50.815496Z","shell.execute_reply":"2025-11-05T09:30:50.88764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary & Final Output\n\n* **Cohort definition:** count of deliveries per **(Year, Course, Method)**. Methods are fixed to **“Face to face”** and **“Live”**. **Year** comes from *Start date resolved* (fallback: *End date resolved*).\n* **Combined table (all years):**\n  `Year, Course Name, Cohorts (Face to face), Cohorts (Live), Participants (Face to face), Participants (Live)`.\n* **Per-year deliverables (3 sheets per year):**\n\n  1. **`YYYY_CourseMethod`** – per-course cohorts & participants split by method.\n  2. **`YYYY_Summaries`** – KPI block (totals, averages, distinct counts, durations) + Monthly and Method breakdowns.\n  3. **`YYYY_Tops`** – Top 10 Courses (sessions & participants), Facilitators (sessions & participants), Countries (participants).\n* **Global sheets:**\n  **`AllYears_CourseMethod`** (full combined table) and **`Yearly_Overview`** (one row per year with key stats & method splits).\n* **Data hygiene:** keep only valid methods, coerce participants to numeric, drop missing Year, add Month from resolved dates.\n* **Output file:** `Training_Performance_By_Year.xlsx`.","metadata":{}},{"cell_type":"code","source":"# === Build per-year tables & summaries (3 sheets per year) and global sheets ===\n# Definitions:\n# - \"Cohort\" = how many times a course was delivered in a calendar year.\n#   Since each row is a delivery/session, cohorts = row count per (Year, Course, Method).\n# - Valid delivery methods are exactly [\"Face to face\", \"Live\"] from the categorical `Method`.\n# - Year comes from 'Year' column (derived from Start date resolved; else End date resolved).\n#\n# Output:\n# - Excel file with:\n#   * For each year Y:\n#       - \"Y_CourseMethod\"\n#       - \"Y_Summaries\"\n#       - \"Y_Tops\"\n#   * Global sheets:\n#       - \"AllYears_CourseMethod\"\n#       - \"Yearly_Overview\"\n\nimport pandas as pd\nimport numpy as np\n\n# --- 0) Start from a clean working copy and ensure 'Year' exists ---\nanalysis_df = course_clean.copy()\n\nif 'Year' not in analysis_df.columns:\n    # Fallback in case this cell is run standalone\n    y_start = analysis_df['Start date resolved'].dt.year\n    y_end   = analysis_df['End date resolved'].dt.year\n    analysis_df['Year'] = y_start.where(y_start.notna(), y_end).astype('Int64')\n\n# Standardize Course Name (use the acronym map from the previous cell if it exists)\nif 'Course Name' not in analysis_df.columns:\n    if 'acronym_map' in globals() and isinstance(acronym_map, dict):\n        analysis_df['Course Name'] = (\n            analysis_df['Course title'].astype(str).str.strip().map(acronym_map)\n            .fillna(analysis_df['Course title'].astype(str).str.strip())\n        )\n    else:\n        analysis_df['Course Name'] = analysis_df['Course title'].astype(str).str.strip()\n\n# Keep only the two valid delivery methods\nvalid_methods = [\"Face to face\", \"Live\"]\nanalysis_df = analysis_df[analysis_df['Method'].isin(valid_methods)].copy()\n\n# Drop rows without a resolvable Year\nanalysis_df = analysis_df.dropna(subset=['Year']).copy()\n\n# Ensure numeric 'Participants count'\nanalysis_df['Participants count'] = pd.to_numeric(analysis_df['Participants count'], errors='coerce').fillna(0)\n\n# Convenience Month column (prefer Start date resolved, else End date resolved)\nmonth_src = analysis_df['Start date resolved'].where(\n    analysis_df['Start date resolved'].notna(), analysis_df['End date resolved']\n)\nanalysis_df['Month'] = pd.to_datetime(month_src).dt.to_period('M').astype(str)  # e.g., \"2025-01\"\n\n# --- 1) Build the combined per-(Year, Course) table with cohorts/participants by Method ---\nby_ycm = (\n    analysis_df\n    .groupby(['Year', 'Course Name', 'Method'], as_index=False)\n    .agg(\n        cohorts=('Course Name', 'size'),\n        participants=('Participants count', 'sum')\n    )\n)\n\n# Pivot wider so each Method has cohorts + participants columns\nwide = by_ycm.pivot_table(\n    index=['Year', 'Course Name'],\n    columns='Method',\n    values=['cohorts', 'participants'],\n    aggfunc='sum',\n    fill_value=0\n)\n\n# Flatten MultiIndex columns with friendly names\nwide.columns = [\n    f\"{'Cohorts' if top=='cohorts' else 'Participants'} ({meth})\"\n    for (top, meth) in wide.columns\n]\nwide = wide.reset_index().sort_values(['Year', 'Course Name'])\n\n# Guarantee expected columns exist even if one method is absent\nexpected_cols = [\n    \"Cohorts (Face to face)\", \"Cohorts (Live)\",\n    \"Participants (Face to face)\", \"Participants (Live)\"\n]\nfor col in expected_cols:\n    if col not in wide.columns:\n        wide[col] = 0\n\nper_year_combined = wide[['Year', 'Course Name'] + expected_cols].copy()\n\n# --- 2) Build a yearly overview summary (one row per year) ---\ndef _safe_nunique(s):\n    return s.dropna().nunique()\n\ndef build_yearly_overview(df):\n    g = df.groupby('Year', dropna=False)\n    out = pd.DataFrame({\n        'total_sessions': g.size(),\n        'total_participants': g['Participants count'].sum(),\n        'distinct_courses': g['Course Name'].apply(_safe_nunique),\n        'distinct_facilitators': g['Facilitator'].apply(_safe_nunique),\n        'distinct_countries': g['Course country'].apply(_safe_nunique),\n        'distinct_cities': g['Course city'].apply(_safe_nunique),\n    })\n    # Duration stats (if available)\n    if 'Duration resolved (days)' in df.columns:\n        out['total_duration_days'] = g['Duration resolved (days)'].sum(min_count=1)\n        out['avg_duration_days']   = g['Duration resolved (days)'].mean()\n    else:\n        out['total_duration_days'] = np.nan\n        out['avg_duration_days']   = np.nan\n\n    # Method splits\n    method_counts = df.groupby(['Year', 'Method']).size().unstack('Method', fill_value=0)\n    method_parts  = df.groupby(['Year', 'Method'])['Participants count'].sum().unstack('Method', fill_value=0)\n    for meth in valid_methods:\n        out[f'sessions_{meth}'] = method_counts.get(meth, 0)\n        out[f'participants_{meth}'] = method_parts.get(meth, 0)\n\n    # Averages\n    out['avg_participants_per_session'] = (out['total_participants'] / out['total_sessions']).replace([np.inf, -np.inf], np.nan)\n    return out.reset_index().sort_values('Year')\n\nyearly_overview = build_yearly_overview(analysis_df)\n\n# --- 3) Per-year extras for the \"Summaries\" and \"Tops\" sheets ---\ndef per_year_kpis(df_y):\n    \"\"\"Return a small KPI table (Metric/Value) for one year.\"\"\"\n    total_sessions = len(df_y)\n    total_participants = int(df_y['Participants count'].sum())\n    distinct_courses = df_y['Course Name'].nunique(dropna=True)\n    distinct_facilitators = df_y['Facilitator'].nunique(dropna=True)\n    distinct_countries = df_y['Course country'].nunique(dropna=True)\n    distinct_cities = df_y['Course city'].nunique(dropna=True)\n\n    if 'Duration resolved (days)' in df_y.columns:\n        total_duration = df_y['Duration resolved (days)'].sum(skipna=True)\n        avg_duration = df_y['Duration resolved (days)'].mean(skipna=True)\n    else:\n        total_duration = np.nan\n        avg_duration = np.nan\n\n    # Method splits\n    ses_by_method = df_y.groupby('Method').size()\n    part_by_method = df_y.groupby('Method')['Participants count'].sum()\n    s_ff = int(ses_by_method.get('Face to face', 0))\n    s_lv = int(ses_by_method.get('Live', 0))\n    p_ff = int(part_by_method.get('Face to face', 0))\n    p_lv = int(part_by_method.get('Live', 0))\n\n    avg_participants_per_session = (total_participants / total_sessions) if total_sessions else np.nan\n\n    data = [\n        ('Total sessions (cohorts)', total_sessions),\n        ('Total participants', total_participants),\n        ('Avg participants per session', round(avg_participants_per_session, 2) if pd.notna(avg_participants_per_session) else np.nan),\n        ('Distinct courses', distinct_courses),\n        ('Distinct facilitators', distinct_facilitators),\n        ('Distinct countries', distinct_countries),\n        ('Distinct cities', distinct_cities),\n        ('Sessions - Face to face', s_ff),\n        ('Sessions - Live', s_lv),\n        ('Participants - Face to face', p_ff),\n        ('Participants - Live', p_lv),\n        ('Total duration (days)', int(total_duration) if pd.notna(total_duration) else np.nan),\n        ('Avg duration (days)', round(avg_duration, 2) if pd.notna(avg_duration) else np.nan),\n    ]\n    return pd.DataFrame(data, columns=['Metric', 'Value'])\n\ndef per_year_monthly(df_y):\n    \"\"\"Return monthly delivery & participant summary for one year.\"\"\"\n    src = df_y['Start date resolved'].where(df_y['Start date resolved'].notna(),\n                                            df_y['End date resolved'])\n    m = pd.to_datetime(src).dt.to_period('M').astype(str)\n    tmp = df_y.copy()\n    tmp['Month'] = m\n    return (tmp\n            .groupby('Month', as_index=True)\n            .agg(Sessions=('Course Name', 'size'),\n                 Participants=('Participants count', 'sum'))\n            .sort_index())\n\ndef per_year_method(df_y):\n    \"\"\"Return method-level summary for one year.\"\"\"\n    return (df_y\n            .groupby('Method', as_index=True)\n            .agg(Sessions=('Method', 'size'),\n                 Participants=('Participants count', 'sum'))\n            .astype(int)\n            .sort_values('Sessions', ascending=False))\n\ndef top_tables(df_y, top_n=10):\n    \"\"\"Return dict of top tables: courses, facilitators, countries (by sessions/participants).\"\"\"\n    top_course_sessions = (df_y.groupby('Course Name')\n                           .size().sort_values(ascending=False).head(top_n)\n                           .rename('Sessions').to_frame())\n    top_course_participants = (df_y.groupby('Course Name')['Participants count']\n                               .sum().sort_values(ascending=False).head(top_n)\n                               .rename('Participants').to_frame())\n\n    top_fac_sessions = (df_y.groupby('Facilitator')\n                        .size().sort_values(ascending=False).head(top_n)\n                        .rename('Sessions').to_frame())\n    top_fac_participants = (df_y.groupby('Facilitator')['Participants count']\n                            .sum().sort_values(ascending=False).head(top_n)\n                            .rename('Participants').to_frame())\n\n    top_country_participants = (df_y.groupby('Course country')['Participants count']\n                                .sum().sort_values(ascending=False).head(top_n)\n                                .rename('Participants').to_frame())\n\n    return {\n        'Top Courses by Sessions': top_course_sessions,\n        'Top Courses by Participants': top_course_participants,\n        'Top Facilitators by Sessions': top_fac_sessions,\n        'Top Facilitators by Participants': top_fac_participants,\n        'Top Countries by Participants': top_country_participants,\n    }\n\n# --- 4) Write everything to Excel (3 sheets per year + global sheets) ---\nout_path = \"Training_Performance_By_Year.xlsx\"\nyears = sorted(int(y) for y in analysis_df['Year'].dropna().unique())\n\nwith pd.ExcelWriter(out_path) as writer:\n    # Global sheets\n    per_year_combined.to_excel(writer, sheet_name='AllYears_CourseMethod', index=False)\n    yearly_overview.to_excel(writer, sheet_name='Yearly_Overview', index=False)\n\n    # Per-year sheets\n    for y in years:\n        df_y = analysis_df[analysis_df['Year'] == y].copy()\n\n        # --- Sheet 1: per-course method table (Y_CourseMethod) ---\n        y_table = (per_year_combined[per_year_combined['Year'] == y]\n                   .drop(columns='Year')\n                   .reset_index(drop=True))\n        y_sheet_name = f\"{y}_CourseMethod\"\n        y_table.to_excel(writer, sheet_name=y_sheet_name, index=False)\n\n        # --- Sheet 2: Summaries (KPIs + Monthly + Method) ---\n        s_sheet = f\"{y}_Summaries\"\n        kpis = per_year_kpis(df_y)\n        kpis.to_excel(writer, sheet_name=s_sheet, index=False, startrow=0, startcol=0)\n\n        # Monthly summary below KPIs\n        monthly = per_year_monthly(df_y)\n        start_row_monthly = len(kpis) + 2\n        monthly.to_excel(writer, sheet_name=s_sheet, startrow=start_row_monthly, startcol=0)\n\n        # Method summary to the right of KPIs\n        method_sum = per_year_method(df_y)\n        start_col_method = 4  # put it a few columns to the right\n        method_sum.to_excel(writer, sheet_name=s_sheet, startrow=0, startcol=start_col_method)\n\n        # --- Sheet 3: Tops (multiple small tables stacked) ---\n        t_sheet = f\"{y}_Tops\"\n        tops = top_tables(df_y, top_n=10)\n        row_cursor = 0\n        for title, tbl in tops.items():\n            # Write a small title row, then the table\n            # We'll write the title as a single-cell DataFrame for simplicity\n            pd.DataFrame({title: []}).to_excel(writer, sheet_name=t_sheet,\n                                               index=False, header=True,\n                                               startrow=row_cursor, startcol=0)\n            row_cursor += 1\n            tbl.to_excel(writer, sheet_name=t_sheet, startrow=row_cursor, startcol=0)\n            row_cursor += len(tbl) + 2  # space before next block\n\nprint(f\"Excel saved to: {out_path}\")\n\n# Optional: quick peek in notebooks\ndisplay(per_year_combined.head())\ndisplay(yearly_overview)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:31:10.435716Z","iopub.execute_input":"2025-11-05T09:31:10.436037Z","iopub.status.idle":"2025-11-05T09:31:10.670229Z","shell.execute_reply.started":"2025-11-05T09:31:10.436013Z","shell.execute_reply":"2025-11-05T09:31:10.669541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Visualization\n\n* **Yearly trend lines**\n\n  1. Total **sessions (cohorts) per year**\n  2. Total **participants per year**\n  3. **Average participants per session** per year\n* **Yearly method splits**\n  4) **Sessions by method per year** (stacked bars: Face to face vs Live)\n  5) **Participants by method per year** (stacked bars)\n* **Per-year details** (loop through each year and generate):\n  6) **Monthly sessions** trend for that year\n  7) **Monthly participants** trend for that year\n  8) **Top 10 courses** by participants for that year (horizontal bars)\n  9) **Top 10 facilitators** by participants for that year (horizontal bars)\n\nAll figures are saved into a `figures/` folder (PNGs) and also shown inline.","metadata":{}},{"cell_type":"code","source":"# Inputs:\n# - Expects `course_clean` DataFrame to exist.\n# - Uses `Year` column if present; otherwise derives from Start/End date resolved.\n# - Valid delivery methods are exactly [\"Face to face\", \"Live\"].\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nfrom pathlib import Path\nimport textwrap\n\n# -----------------------------\n# Helper utilities\n# -----------------------------\ndef fmt_thousands(x, pos=None):\n    \"\"\"Axis formatter with thousands separators and no decimals.\"\"\"\n    try:\n        return f\"{int(x):,}\"\n    except Exception:\n        return str(x)\n\ndef fmt_decimal(x, decimals=2):\n    \"\"\"Format a float to N decimals with thousands separators.\"\"\"\n    if pd.isna(x):\n        return \"\"\n    return f\"{x:,.{decimals}f}\"\n\ndef add_headroom(ax, pct=0.05):\n    \"\"\"Add headroom to the upper y-limit for nicer label spacing.\"\"\"\n    ymin, ymax = ax.get_ylim()\n    if np.isfinite(ymax):\n        ax.set_ylim(ymin, ymax * (1 + pct if ymax > 0 else 1 - pct))\n\ndef label_line_points(ax, x_vals, y_vals, decimals=0):\n    \"\"\"Label each point on a line slightly above it.\"\"\"\n    for xv, yv in zip(x_vals, y_vals):\n        if pd.isna(yv):\n            continue\n        ax.annotate(\n            fmt_decimal(yv, decimals),\n            xy=(xv, yv),\n            xytext=(0, 6),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=9,\n        )\n\ndef label_yoy_change(ax, x_vals, y_vals):\n    \"\"\"Annotate YoY % change next to each point (from previous point).\"\"\"\n    for i in range(1, len(x_vals)):\n        prev = y_vals[i - 1]\n        curr = y_vals[i]\n        if pd.isna(prev) or prev == 0 or pd.isna(curr):\n            continue\n        pct = (curr / prev - 1.0) * 100.0\n        # place slightly below the point to avoid clashing with the value label\n        ax.annotate(\n            f\"{pct:+.1f}%\",\n            xy=(x_vals[i], y_vals[i]),\n            xytext=(0, -14),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"top\",\n            fontsize=9,\n        )\n\ndef wrap_or_truncate(label, width=28, max_lines=2):\n    \"\"\"Wrap long labels to multiple lines; if still too long, truncate last line with ellipsis.\"\"\"\n    if not isinstance(label, str):\n        return label\n    wrapped = textwrap.wrap(label, width=width)\n    if len(wrapped) <= max_lines:\n        return \"\\n\".join(wrapped)\n    keep = wrapped[: max_lines - 1]\n    last = wrapped[max_lines - 1]\n    if len(last) > width:\n        last = last[: max(0, width - 1)] + \"…\"\n    keep.append(last)\n    return \"\\n\".join(keep)\n\ndef label_bar_values(ax):\n    \"\"\"Attach value labels to bars (vertical or horizontal).\"\"\"\n    for container in ax.containers:\n        try:\n            ax.bar_label(container, fmt=\"{}\")\n        except Exception:\n            pass\n\ndef label_stacked_segments_with_share(ax, stacked_df):\n    \"\"\"\n    For stacked bars: label each segment as 'value (xx%)'.\n    stacked_df: DataFrame indexed by x positions (years), columns = segments\n    \"\"\"\n    totals = stacked_df.sum(axis=1).values\n    for c_idx, container in enumerate(ax.containers):\n        bars = list(container)\n        for i, bar in enumerate(bars):\n            height = bar.get_height()\n            total = totals[i] if i < len(totals) else 0\n            if total > 0 and height > 0:\n                pct = height / total * 100.0\n                label = f\"{int(height):,} ({pct:.0f}%)\"\n                ax.annotate(\n                    label,\n                    xy=(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n                    ha=\"center\",\n                    va=\"center\",\n                    fontsize=9,\n                )\n\ndef add_stacked_totals(ax, x_positions, totals):\n    \"\"\"Add total labels on top of stacked bars.\"\"\"\n    ymin, ymax = ax.get_ylim()\n    y_offset = (ymax - ymin) * 0.01\n    for x, t in zip(x_positions, totals):\n        ax.annotate(\n            f\"{int(t):,}\",\n            xy=(x, t),\n            xytext=(0, 2),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=10,\n            fontweight=\"bold\",\n        )\n\n# -----------------------------\n# 0) Prepare data safely\n# -----------------------------\ndf = course_clean.copy()\n\n# Ensure 'Year'\nif \"Year\" not in df.columns:\n    y_start = df[\"Start date resolved\"].dt.year\n    y_end = df[\"End date resolved\"].dt.year\n    df[\"Year\"] = y_start.where(y_start.notna(), y_end).astype(\"Int64\")\n\n# Standardize 'Course Name' (if not already created)\nif \"Course Name\" not in df.columns:\n    if \"acronym_map\" in globals() and isinstance(acronym_map, dict):\n        df[\"Course Name\"] = (\n            df[\"Course title\"].astype(str).str.strip().map(acronym_map)\n            .fillna(df[\"Course title\"].astype(str).str.strip())\n        )\n    else:\n        df[\"Course Name\"] = df[\"Course title\"].astype(str).str.strip()\n\n# Keep only valid methods\nvalid_methods = [\"Face to face\", \"Live\"]\ndf = df[df[\"Method\"].isin(valid_methods)].copy()\n\n# Drop rows without a year\ndf = df.dropna(subset=[\"Year\"]).copy()\n\n# Numeric participants\ndf[\"Participants count\"] = pd.to_numeric(df[\"Participants count\"], errors=\"coerce\").fillna(0)\n\n# Month per row (prefer Start date resolved; else End date resolved)\ndate_src = df[\"Start date resolved\"].where(df[\"Start date resolved\"].notna(), df[\"End date resolved\"])\ndf[\"Month\"] = pd.to_datetime(date_src).dt.to_period(\"M\").astype(str)  # \"YYYY-MM\"\ndf[\"Month_Num\"] = pd.to_datetime(date_src).dt.month                   # 1..12\n\n# Sorted years\nyears = sorted(int(y) for y in df[\"Year\"].dropna().unique())\n\n# Output folder\nfig_dir = Path(\"figures_better\")\nfig_dir.mkdir(exist_ok=True)\n\n# Common number formatter on axes\nthousands_formatter = FuncFormatter(fmt_thousands)\n\n# -----------------------------\n# (A) All-year trend charts (with labels + YoY)\n# -----------------------------\nyearly_sessions = df.groupby(\"Year\").size().astype(int)\nyearly_participants = df.groupby(\"Year\")[\"Participants count\"].sum().astype(int)\nyearly_avg_pps = (yearly_participants / yearly_sessions).replace([np.inf, -np.inf], np.nan)\n\n# 1) Sessions per year\nx_years = yearly_sessions.index.astype(int).tolist()\ny_vals = yearly_sessions.values.astype(float).tolist()\nplt.figure(figsize=(10, 5), dpi=150)\nax = plt.gca()\nax.plot(x_years, y_vals, marker=\"o\")\nax.set_title(\"Total Sessions (Cohorts) per Year\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Sessions (cohorts)\")\nax.grid(True, alpha=0.3)\nax.yaxis.set_major_formatter(thousands_formatter)\nlabel_line_points(ax, x_years, y_vals, decimals=0)\nlabel_yoy_change(ax, x_years, y_vals)\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"all_years_sessions.png\")\nplt.show()\n\n# 2) Participants per year\nx_years = yearly_participants.index.astype(int).tolist()\ny_vals = yearly_participants.values.astype(float).tolist()\nplt.figure(figsize=(10, 5), dpi=150)\nax = plt.gca()\nax.plot(x_years, y_vals, marker=\"o\")\nax.set_title(\"Total Participants per Year\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Participants\")\nax.grid(True, alpha=0.3)\nax.yaxis.set_major_formatter(thousands_formatter)\nlabel_line_points(ax, x_years, y_vals, decimals=0)\nlabel_yoy_change(ax, x_years, y_vals)\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"all_years_participants.png\")\nplt.show()\n\n# 3) Avg participants per session\nx_years = yearly_avg_pps.index.astype(int).tolist()\ny_vals = yearly_avg_pps.values.astype(float).tolist()\nplt.figure(figsize=(10, 5), dpi=150)\nax = plt.gca()\nax.plot(x_years, y_vals, marker=\"o\")\nax.set_title(\"Average Participants per Session by Year\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Avg participants / session\")\nax.grid(True, alpha=0.3)\nlabel_line_points(ax, x_years, y_vals, decimals=2)\nlabel_yoy_change(ax, x_years, y_vals)\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"all_years_avg_pps.png\")\nplt.show()\n\n# 4) Sessions by method per year (stacked) with segment shares + total\nsessions_method = (\n    df.groupby([\"Year\", \"Method\"])\n      .size().unstack(\"Method\", fill_value=0)\n      .reindex(index=years)\n)\nplt.figure(figsize=(10, 6), dpi=150)\nax = plt.gca()\nbottom = np.zeros(len(sessions_method))\nx_pos = np.array(sessions_method.index.astype(int))\nfor meth in valid_methods:\n    vals = sessions_method[meth].values if meth in sessions_method.columns else np.zeros(len(sessions_method))\n    ax.bar(x_pos, vals, bottom=bottom, label=meth)\n    bottom = bottom + vals\nax.set_title(\"Sessions by Method per Year (Stacked)\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Sessions\")\nax.yaxis.set_major_formatter(thousands_formatter)\nax.legend()\nlabel_stacked_segments_with_share(ax, sessions_method)\nadd_stacked_totals(ax, x_pos, sessions_method.sum(axis=1).values)\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"all_years_sessions_by_method.png\")\nplt.show()\n\n# 5) Participants by method per year (stacked) with segment shares + total\nparticipants_method = (\n    df.groupby([\"Year\", \"Method\"])[\"Participants count\"]\n      .sum().unstack(\"Method\", fill_value=0)\n      .reindex(index=years)\n)\nplt.figure(figsize=(10, 6), dpi=150)\nax = plt.gca()\nbottom = np.zeros(len(participants_method))\nx_pos = np.array(participants_method.index.astype(int))\nfor meth in valid_methods:\n    vals = participants_method[meth].values if meth in participants_method.columns else np.zeros(len(participants_method))\n    ax.bar(x_pos, vals, bottom=bottom, label=meth)\n    bottom = bottom + vals\nax.set_title(\"Participants by Method per Year (Stacked)\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Participants\")\nax.yaxis.set_major_formatter(thousands_formatter)\nax.legend()\nlabel_stacked_segments_with_share(ax, participants_method)\nadd_stacked_totals(ax, x_pos, participants_method.sum(axis=1).values)\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"all_years_participants_by_method.png\")\nplt.show()\n\n# -----------------------------\n# (B) NEW: Combined monthly charts (one figure each) — multiple years as separate lines\n# -----------------------------\n# Sessions per month across years (lines by year)\nmonthly_sessions_by_year = (\n    df.groupby([\"Year\", \"Month_Num\"])\n      .size()\n      .rename(\"Sessions\")\n      .reset_index()\n)\n\n# Ensure all months 1..12 exist per year (fill missing with 0)\nall_months = pd.DataFrame({\"Month_Num\": np.arange(1, 13, dtype=int)})\nlines_sessions = {}\nfor y in years:\n    temp = monthly_sessions_by_year[monthly_sessions_by_year[\"Year\"] == y][[\"Month_Num\", \"Sessions\"]]\n    temp = all_months.merge(temp, on=\"Month_Num\", how=\"left\").fillna(0)\n    lines_sessions[y] = temp[\"Sessions\"].astype(float).tolist()\n\nplt.figure(figsize=(12, 6), dpi=150)\nax = plt.gca()\nmonths_xticks = np.arange(1, 13, dtype=int)\nfor y in years:\n    ax.plot(months_xticks, lines_sessions[y], marker=\"o\", label=str(y))\n# Labels on points for the last year only (to reduce clutter) or label all years if desired:\n# for y in years:\n#     label_line_points(ax, months_xticks, lines_sessions[y], decimals=0)\n# Here: label the latest year prominently\nif len(years) > 0:\n    latest = years[-1]\n    label_line_points(ax, months_xticks, lines_sessions[latest], decimals=0)\n\nax.set_title(\"Monthly Sessions by Year\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Sessions\")\nax.set_xticks(months_xticks)\nax.set_xticklabels([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\nax.yaxis.set_major_formatter(thousands_formatter)\nax.grid(True, alpha=0.3)\nax.legend(title=\"Year\", ncol=min(len(years), 6))\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"combined_monthly_sessions_by_year.png\")\nplt.show()\n\n# Participants per month across years (lines by year)\nmonthly_participants_by_year = (\n    df.groupby([\"Year\", \"Month_Num\"])[\"Participants count\"]\n      .sum()\n      .rename(\"Participants\")\n      .reset_index()\n)\n\nlines_participants = {}\nfor y in years:\n    temp = monthly_participants_by_year[monthly_participants_by_year[\"Year\"] == y][[\"Month_Num\", \"Participants\"]]\n    temp = all_months.merge(temp, on=\"Month_Num\", how=\"left\").fillna(0)\n    lines_participants[y] = temp[\"Participants\"].astype(float).tolist()\n\nplt.figure(figsize=(12, 6), dpi=150)\nax = plt.gca()\nfor y in years:\n    ax.plot(months_xticks, lines_participants[y], marker=\"o\", label=str(y))\n# Label only the latest year's points to avoid clutter\nif len(years) > 0:\n    latest = years[-1]\n    label_line_points(ax, months_xticks, lines_participants[latest], decimals=0)\n\nax.set_title(\"Monthly Participants by Year\")\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Participants\")\nax.set_xticks(months_xticks)\nax.set_xticklabels([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\nax.yaxis.set_major_formatter(thousands_formatter)\nax.grid(True, alpha=0.3)\nax.legend(title=\"Year\", ncol=min(len(years), 6))\nadd_headroom(ax, pct=0.08)\nplt.tight_layout()\nplt.savefig(fig_dir / \"combined_monthly_participants_by_year.png\")\nplt.show()\n\n# -----------------------------\n# (C) Per-year TOPS (kept as before)\n# -----------------------------\nfor y in years:\n    df_y = df[df[\"Year\"] == y].copy()\n\n    # Top 10 courses by participants (horizontal bars + wrapped names + labels)\n    top_courses = (\n        df_y.groupby(\"Course Name\")[\"Participants count\"]\n            .sum().sort_values(ascending=False).head(10)\n    )\n    labels_wrapped = [wrap_or_truncate(s) for s in top_courses.index[::-1]]\n    plt.figure(figsize=(11, 6), dpi=150)\n    ax = plt.gca()\n    ax.barh(labels_wrapped, top_courses.values[::-1])\n    ax.set_title(f\"Top 10 Courses by Participants in {y}\")\n    ax.set_xlabel(\"Participants\")\n    ax.set_ylabel(\"Course\")\n    ax.xaxis.set_major_formatter(thousands_formatter)\n    label_bar_values(ax)\n    plt.tight_layout()\n    plt.savefig(fig_dir / f\"{y}_top10_courses_participants.png\")\n    plt.show()\n\n    # Top 10 facilitators by participants (horizontal bars + wrapped names + labels)\n    top_fac = (\n        df_y.groupby(\"Facilitator\")[\"Participants count\"]\n            .sum().sort_values(ascending=False).head(10)\n    )\n    labels_wrapped = [wrap_or_truncate(s) for s in top_fac.index[::-1]]\n    plt.figure(figsize=(11, 6), dpi=150)\n    ax = plt.gca()\n    ax.barh(labels_wrapped, top_fac.values[::-1])\n    ax.set_title(f\"Top 10 Facilitators by Participants in {y}\")\n    ax.set_xlabel(\"Participants\")\n    ax.set_ylabel(\"Facilitator\")\n    ax.xaxis.set_major_formatter(thousands_formatter)\n    label_bar_values(ax)\n    plt.tight_layout()\n    plt.savefig(fig_dir / f\"{y}_top10_facilitators_participants.png\")\n    plt.show()\n\nprint(f\"Saved figures to: {fig_dir.resolve()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T09:31:34.159179Z","iopub.execute_input":"2025-11-05T09:31:34.160103Z","iopub.status.idle":"2025-11-05T09:31:40.930543Z","shell.execute_reply.started":"2025-11-05T09:31:34.160062Z","shell.execute_reply":"2025-11-05T09:31:40.929743Z"}},"outputs":[],"execution_count":null}]}